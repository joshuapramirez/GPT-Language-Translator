{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb10b915-e513-40d4-852d-0b087d987aa0",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2rem;color:green;\">English to Portuguese Translator LLM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bd07-18ce-4aca-9214-fe217ca94335",
   "metadata": {},
   "source": [
    "## Set up PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33695f12-9591-4444-a964-3c1132c3d734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\joshu\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\joshu\\anaconda3\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\joshu\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\joshu\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\joshu\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipywidgets) (6.25.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: widgetsnbextension in c:\\users\\joshu\\anaconda3\\lib\\site-packages (4.0.5)\n",
      "Requirement already satisfied: jupyter_contrib_nbextensions in c:\\users\\joshu\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jupyter-contrib-core>=0.3.3 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (0.4.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (5.5.0)\n",
      "Requirement already satisfied: jupyter-highlight-selected-word>=0.1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jupyter-nbextensions-configurator>=0.4.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (0.6.3)\n",
      "Requirement already satisfied: nbconvert>=6.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (7.10.0)\n",
      "Requirement already satisfied: notebook>=6.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (7.0.6)\n",
      "Requirement already satisfied: tornado in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=4.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (5.7.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter_contrib_nbextensions) (4.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-contrib-core>=0.3.3->jupyter_contrib_nbextensions) (68.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-nbextensions-configurator>=0.4.0->jupyter_contrib_nbextensions) (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (5.9.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (23.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (1.2.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-core->jupyter_contrib_nbextensions) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-core->jupyter_contrib_nbextensions) (305.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (4.0.8)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (0.2.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (3.5.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (8.6.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.4.4)\n",
      "Requirement already satisfied: overrides in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.14.1)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (25.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.58.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (6.25.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.31.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nbformat>=5.7->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.16.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2023.3.post1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.8.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2023.11.17)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (21.2.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (8.20.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (1.5.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (5.9.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.4.6)\n",
      "Requirement already satisfied: fqdn in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.2.3)\n",
      "Requirement already satisfied: executing in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Scripts\\jupyter-contrib.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\jupyter_core\\application.py\", line 280, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 990, in launch_instance\n",
      "    app = cls.instance(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\traitlets\\config\\configurable.py\", line 552, in instance\n",
      "    inst = cls(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 27, in __init__\n",
      "    self._refresh_subcommands()\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\jupyter_contrib_core\\application.py\", line 43, in _refresh_subcommands\n",
      "    get_subcommands_dict = entrypoint.load()\n",
      "                           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2518, in load\n",
      "    return self.resolve()\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2524, in resolve\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\jupyter_contrib_nbextensions\\__init__.py\", line 5, in <module>\n",
      "    import jupyter_nbextensions_configurator\n",
      "  File \"C:\\Users\\joshu\\anaconda3\\Lib\\site-packages\\jupyter_nbextensions_configurator\\__init__.py\", line 18, in <module>\n",
      "    from notebook.base.handlers import APIHandler, IPythonHandler\n",
      "ModuleNotFoundError: No module named 'notebook.base'\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console contrib dejavu events execute kernel kernelspec\n",
      "lab labextension labhub migrate nbconvert nbextensions_configurator notebook\n",
      "qtconsole run script server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas\n",
    "!pip install ipywidgets\n",
    "!pip install widgetsnbextension\n",
    "!pip install jupyter_contrib_nbextensions\n",
    "!jupyter contrib nbextension install --user\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2186ad4-7342-4ec6-b168-97a93b66af7f",
   "metadata": {},
   "source": [
    "## 1. Import the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b80005-deb8-4b0b-9174-b4832e9e5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_2716\\547781205.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689cd82-b256-4d28-b9a2-18fca3c8c3b8",
   "metadata": {},
   "source": [
    "## 2. Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6116b6d4-9cab-42e5-a5e8-9ea303cb955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            _MASKING_VALUE = -1e+30 if attn_scores.dtype == torch.float32 else -1e+4            \n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, value=_MASKING_VALUE)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a03d9a-3674-4a5f-b02d-9cb21b7e89a0",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd96f8ee-42a2-4ece-852d-42f7988dc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81f4f9-a0a1-4b94-a121-7dbb1c2dbac0",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab59428-74c5-49af-a45a-214b5601c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length  # Store max_seq_length explicitly\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f'x size: {x.size()}')\n",
    "        # print(f'self.pe size: {self.pe.size()}')\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45275168-5ae2-41c3-98e7-2e966d126851",
   "metadata": {},
   "source": [
    "## 3. Building the Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86792e14-9ce9-4392-b3fb-ef30c4b1d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d2041-cc0e-45fb-bfc9-93d5a548c74b",
   "metadata": {},
   "source": [
    "## 4. Building the Decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a928c711-0279-4216-9c2b-019c88045e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc0d17-3d30-4a4f-a6d2-2469b72be26c",
   "metadata": {},
   "source": [
    "## 5. Combine the Encoder and Decoder layers to complete the Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75509b7c-d556-4fb3-aa97-cc6d2ae9aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        # Ensure tgt_mask is on the same device as tgt\n",
    "        #tgt_mask = tgt_mask.to(tgt.device)\n",
    "\n",
    "        nopeak_mask = nopeak_mask.to(tgt.device)\n",
    "        \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        tgt_on_cuda = tgt_mask.is_cuda\n",
    "        nopeak_on_cuda = nopeak_mask.is_cuda\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950fe9c-a625-4cf0-98cd-9c1b0496ad2f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2rem;color:green;\">Dataset Loading and Tokenization, and Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdcdf7-6ec4-4be2-b152-456a2f9b4427",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80d8880-fd5f-45a6-a523-cabd21e5bd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: torchtext==0.6.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\joshu\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joshu\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joshu\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchtext==0.6.0) (2.31.0)\n",
      "Requirement already satisfied: torch in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchtext==0.6.0) (2.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torchtext==0.6.0) (0.1.99)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6.0) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6.0) (3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6.0) (2023.10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\joshu\\anaconda3\\lib\\site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.6.0 spacy numpy nltk scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4da134f-b4f3-4529-b8c0-6b8570cbb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer, Field, TabularDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d58c5-a03d-4597-9e62-f8e625dd7275",
   "metadata": {},
   "source": [
    "# Load English to Portuguese Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eba007-4cf8-44a8-ba48-f91200fbec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vai.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vá.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Oi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corre!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN      PT\n",
       "0   Go.    Vai.\n",
       "1   Go.     Vá.\n",
       "2   Hi.     Oi.\n",
       "3  Run!  Corre!\n",
       "4  Run!  Corra!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English to Portuguese translation data\n",
    "dataset_file_path = './por.txt'  # Adjust the path accordingly\n",
    "\n",
    "# Load and preprocess translation data\n",
    "df = pd.read_csv(dataset_file_path, sep='\\t', header=None)[[0, 1]].rename(columns={0: 'EN', 1: 'PT'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d09e0db-d466-41aa-b39a-21f50ab1bc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168903, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39734877-7322-4b74-8100-50ef9ffc28b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tom was livid.</td>\n",
       "      <td>Tom estava pálido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom's here.</td>\n",
       "      <td>Tom está aqui.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom died just three days after Mary was born.</td>\n",
       "      <td>Tom faleceu apenas três dias após Maria nascer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't accept your gift.</td>\n",
       "      <td>Não posso aceitar o seu presente.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm not afraid to die.</td>\n",
       "      <td>Eu não tenho medo de morrer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              EN  \\\n",
       "0                                 Tom was livid.   \n",
       "1                                    Tom's here.   \n",
       "2  Tom died just three days after Mary was born.   \n",
       "3                      I can't accept your gift.   \n",
       "4                         I'm not afraid to die.   \n",
       "\n",
       "                                                PT  \n",
       "0                               Tom estava pálido.  \n",
       "1                                   Tom está aqui.  \n",
       "2  Tom faleceu apenas três dias após Maria nascer.  \n",
       "3                Não posso aceitar o seu presente.  \n",
       "4                     Eu não tenho medo de morrer.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the rows and reindex\n",
    "df = df.sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf73afb-6268-4b80-88ba-c85077c2fda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:100000]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8386ae48-c4d3-4279-9b57-dfc8be7fc7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tom was livid.</td>\n",
       "      <td>&lt;SOS&gt; Tom estava pálido. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom's here.</td>\n",
       "      <td>&lt;SOS&gt; Tom está aqui. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom died just three days after Mary was born.</td>\n",
       "      <td>&lt;SOS&gt; Tom faleceu apenas três dias após Maria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't accept your gift.</td>\n",
       "      <td>&lt;SOS&gt; Não posso aceitar o seu presente. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm not afraid to die.</td>\n",
       "      <td>&lt;SOS&gt; Eu não tenho medo de morrer. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              EN  \\\n",
       "0                                 Tom was livid.   \n",
       "1                                    Tom's here.   \n",
       "2  Tom died just three days after Mary was born.   \n",
       "3                      I can't accept your gift.   \n",
       "4                         I'm not afraid to die.   \n",
       "\n",
       "                                                  PT  \n",
       "0                     <SOS> Tom estava pálido. <EOS>  \n",
       "1                         <SOS> Tom está aqui. <EOS>  \n",
       "2  <SOS> Tom faleceu apenas três dias após Maria ...  \n",
       "3      <SOS> Não posso aceitar o seu presente. <EOS>  \n",
       "4           <SOS> Eu não tenho medo de morrer. <EOS>  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Extra Context for Tranformer\n",
    "df['PT'] = df['PT'].apply(lambda x: \"<SOS> \" + x + \" <EOS>\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4bc8b-436e-48e2-9692-03d959ae175b",
   "metadata": {},
   "source": [
    "# Preprocess/Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9db7008-09be-4ef9-882d-eee788b8d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(re.compile(r'[^a-zA-Z0-9\\s]'), '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = \" \".join([i.strip() for i in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def pt_preprocess(text):\n",
    "    text = text.replace(\"\\u202f\",\" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub(re.compile(\"[^a-zéâàçêêëôîû!?',]\"), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8cafba9-9648-4068-9a62-04fbfb8f1eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tom was livid</td>\n",
       "      <td>&lt;SOS&gt; Tom estava pálido. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toms here</td>\n",
       "      <td>&lt;SOS&gt; Tom está aqui. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tom died just three days after mary was born</td>\n",
       "      <td>&lt;SOS&gt; Tom faleceu apenas três dias após Maria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cant accept your gift</td>\n",
       "      <td>&lt;SOS&gt; Não posso aceitar o seu presente. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im not afraid to die</td>\n",
       "      <td>&lt;SOS&gt; Eu não tenho medo de morrer. &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             EN  \\\n",
       "0                                 tom was livid   \n",
       "1                                     toms here   \n",
       "2  tom died just three days after mary was born   \n",
       "3                       i cant accept your gift   \n",
       "4                          im not afraid to die   \n",
       "\n",
       "                                                  PT  \n",
       "0                     <SOS> Tom estava pálido. <EOS>  \n",
       "1                         <SOS> Tom está aqui. <EOS>  \n",
       "2  <SOS> Tom faleceu apenas três dias após Maria ...  \n",
       "3      <SOS> Não posso aceitar o seu presente. <EOS>  \n",
       "4           <SOS> Eu não tenho medo de morrer. <EOS>  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EN'] = df['EN'].apply(lambda x: eng_preprocess(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20761db9-4273-47ee-8bda-3ab856b6fa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tom was livid</td>\n",
       "      <td>sos  tom estava p lido   eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toms here</td>\n",
       "      <td>sos  tom est  aqui   eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tom died just three days after mary was born</td>\n",
       "      <td>sos  tom faleceu apenas três dias ap s maria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cant accept your gift</td>\n",
       "      <td>sos  n o posso aceitar o seu presente   eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im not afraid to die</td>\n",
       "      <td>sos  eu n o tenho medo de morrer   eos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             EN  \\\n",
       "0                                 tom was livid   \n",
       "1                                     toms here   \n",
       "2  tom died just three days after mary was born   \n",
       "3                       i cant accept your gift   \n",
       "4                          im not afraid to die   \n",
       "\n",
       "                                                  PT  \n",
       "0                      sos  tom estava p lido   eos   \n",
       "1                          sos  tom est  aqui   eos   \n",
       "2   sos  tom faleceu apenas três dias ap s maria ...  \n",
       "3       sos  n o posso aceitar o seu presente   eos   \n",
       "4            sos  eu n o tenho medo de morrer   eos   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PT'] = df['PT'].apply(lambda x: pt_preprocess(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d51b47-0b9a-401e-ae77-f11a6804a64c",
   "metadata": {},
   "source": [
    "# Tokenize Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75d2af8-b180-4a99-9079-56692a650b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tokenization(feat):\n",
    "    # Step 1: Create a tokenizer\n",
    "    tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "    # Step 2: Create a torchtext Field for English features\n",
    "    eng_field = Field(tokenize=tokenizer, lower=True)\n",
    "\n",
    "    # Step 3: Use the Field to process the feature text\n",
    "    eng_field.build_vocab([eng_field.preprocess(f) for f in feat])\n",
    "\n",
    "    # Step 4: Convert feature text to sequences\n",
    "    eng_seq = [eng_field.numericalize([f]) for f in feat]\n",
    "\n",
    "    # Step 5: Convert PyTorch tensors to Python lists\n",
    "    eng_seq = [seq.squeeze().tolist() for seq in eng_seq]\n",
    "\n",
    "    return eng_seq, eng_field\n",
    "\n",
    "def target_tokenization(target):\n",
    "    # Step 1: Create a tokenizer\n",
    "    tokenizer = get_tokenizer(\"spacy\", language=\"pt_core_news_sm\")\n",
    "\n",
    "    # Step 2: Create a torchtext Field for Portuguese targets\n",
    "    port_field = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "    # Step 3: Use the Field to process the target text\n",
    "    port_field.build_vocab([port_field.preprocess(t) for t in target])\n",
    "\n",
    "    # Step 4: Convert target text to sequences\n",
    "    port_seq = [port_field.numericalize([t]) for t in target]\n",
    "\n",
    "    # Step 5: Convert PyTorch tensors to Python lists\n",
    "    port_seq = [seq.squeeze().tolist() for seq in port_seq]\n",
    "\n",
    "    return port_seq, port_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8ad18a-39a3-4626-bcd5-10bd3a9dbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sequences, eng_field = feature_tokenization(df['EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad545b8e-aaf1-4151-97d4-feeea14c4124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10783"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab = len(eng_field.vocab) + 1\n",
    "eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8399e7a9-c5ae-4ad0-b037-eff8db83c86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 24, 0, 0, 10, 34, 0, 0, 2, 0, 2, 81],\n",
       " [0, 0, 24, 34, 0, 0, 0, 0, 0],\n",
       " [0,\n",
       "  0,\n",
       "  24,\n",
       "  0,\n",
       "  81,\n",
       "  2,\n",
       "  0,\n",
       "  81,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  34,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  81,\n",
       "  10,\n",
       "  0,\n",
       "  34,\n",
       "  0,\n",
       "  10,\n",
       "  8392,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  24,\n",
       "  10,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  10,\n",
       "  34,\n",
       "  0,\n",
       "  3704,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2,\n",
       "  0,\n",
       "  2886,\n",
       "  10,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  10,\n",
       "  2886,\n",
       "  2886,\n",
       "  0,\n",
       "  9507,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  8392,\n",
       "  0],\n",
       " [2, 24, 0, 0, 0, 0, 0, 10, 8392, 0, 10, 2, 81, 0, 0, 0, 0, 81, 2, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aadfb6b-71fb-4bab-82eb-a6db4a69ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_sequences, port_field = target_tokenization(df['PT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "307d1051-b56d-4320-8d29-e4865b3d76bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16450"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_vocab = len(port_field.vocab) + 1\n",
    "port_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cda22b41-02d2-40f8-89e7-6b91a8a1058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  15,\n",
       "  125,\n",
       "  15,\n",
       "  4,\n",
       "  257,\n",
       "  4,\n",
       "  94,\n",
       "  1106,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  4,\n",
       "  4,\n",
       "  15,\n",
       "  0,\n",
       "  4921,\n",
       "  1106,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  229,\n",
       "  15,\n",
       "  94,\n",
       "  35,\n",
       "  430,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  35,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  82,\n",
       "  230,\n",
       "  0,\n",
       "  23,\n",
       "  4,\n",
       "  196,\n",
       "  1106,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  4,\n",
       "  23,\n",
       "  4,\n",
       "  83,\n",
       "  15,\n",
       "  230,\n",
       "  1106,\n",
       "  15,\n",
       "  4,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  430,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  257,\n",
       "  8,\n",
       "  23,\n",
       "  23,\n",
       "  8,\n",
       "  4,\n",
       "  15,\n",
       "  430,\n",
       "  35,\n",
       "  1106,\n",
       "  82,\n",
       "  15,\n",
       "  230,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  23,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  257,\n",
       "  230,\n",
       "  35,\n",
       "  23,\n",
       "  35,\n",
       "  11,\n",
       "  82,\n",
       "  35,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  82,\n",
       "  35,\n",
       "  11,\n",
       "  77,\n",
       "  8,\n",
       "  4,\n",
       "  83,\n",
       "  35,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  196,\n",
       "  35,\n",
       "  4,\n",
       "  83,\n",
       "  8,\n",
       "  230,\n",
       "  230,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71755b5e-f40a-4132-8785-3f26575314f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  15,\n",
       "  125,\n",
       "  15,\n",
       "  4,\n",
       "  257,\n",
       "  4,\n",
       "  94,\n",
       "  1106,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  4,\n",
       "  4,\n",
       "  15,\n",
       "  0,\n",
       "  4921,\n",
       "  1106,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  229,\n",
       "  15,\n",
       "  94,\n",
       "  35,\n",
       "  430,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  35,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  82,\n",
       "  230,\n",
       "  0,\n",
       "  23,\n",
       "  4,\n",
       "  196,\n",
       "  1106,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  4,\n",
       "  23,\n",
       "  4,\n",
       "  83,\n",
       "  15,\n",
       "  230,\n",
       "  1106,\n",
       "  15,\n",
       "  4,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  430,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  257,\n",
       "  8,\n",
       "  23,\n",
       "  23,\n",
       "  8,\n",
       "  4,\n",
       "  15,\n",
       "  430,\n",
       "  35,\n",
       "  1106,\n",
       "  82,\n",
       "  15,\n",
       "  230,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  23,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  257,\n",
       "  230,\n",
       "  35,\n",
       "  23,\n",
       "  35,\n",
       "  11,\n",
       "  82,\n",
       "  35,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23],\n",
       " [4,\n",
       "  23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  82,\n",
       "  35,\n",
       "  11,\n",
       "  77,\n",
       "  8,\n",
       "  4,\n",
       "  83,\n",
       "  35,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  196,\n",
       "  35,\n",
       "  4,\n",
       "  83,\n",
       "  8,\n",
       "  230,\n",
       "  230,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_inp = [x[:-1] for x in port_sequences]\n",
    "port_inp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab112cea-74f7-4598-b67d-e0357ac22b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  15,\n",
       "  125,\n",
       "  15,\n",
       "  4,\n",
       "  257,\n",
       "  4,\n",
       "  94,\n",
       "  1106,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  35,\n",
       "  23,\n",
       "  82,\n",
       "  4,\n",
       "  4,\n",
       "  15,\n",
       "  0,\n",
       "  4921,\n",
       "  1106,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  82,\n",
       "  8,\n",
       "  83,\n",
       "  4,\n",
       "  229,\n",
       "  15,\n",
       "  94,\n",
       "  35,\n",
       "  430,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  35,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  82,\n",
       "  230,\n",
       "  0,\n",
       "  23,\n",
       "  4,\n",
       "  196,\n",
       "  1106,\n",
       "  15,\n",
       "  23,\n",
       "  4,\n",
       "  15,\n",
       "  257,\n",
       "  4,\n",
       "  23,\n",
       "  4,\n",
       "  83,\n",
       "  15,\n",
       "  230,\n",
       "  1106,\n",
       "  15,\n",
       "  4,\n",
       "  11,\n",
       "  15,\n",
       "  23,\n",
       "  430,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  257,\n",
       "  8,\n",
       "  23,\n",
       "  23,\n",
       "  8,\n",
       "  4,\n",
       "  15,\n",
       "  430,\n",
       "  35,\n",
       "  1106,\n",
       "  82,\n",
       "  15,\n",
       "  230,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  23,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  257,\n",
       "  230,\n",
       "  35,\n",
       "  23,\n",
       "  35,\n",
       "  11,\n",
       "  82,\n",
       "  35,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4],\n",
       " [23,\n",
       "  8,\n",
       "  23,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  4921,\n",
       "  4,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  82,\n",
       "  35,\n",
       "  11,\n",
       "  77,\n",
       "  8,\n",
       "  4,\n",
       "  83,\n",
       "  35,\n",
       "  196,\n",
       "  8,\n",
       "  4,\n",
       "  196,\n",
       "  35,\n",
       "  4,\n",
       "  83,\n",
       "  8,\n",
       "  230,\n",
       "  230,\n",
       "  35,\n",
       "  230,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  35,\n",
       "  8,\n",
       "  23,\n",
       "  4]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_out = [x[1:] for x in port_sequences]\n",
    "port_out[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cee15-64b6-4e48-b332-65e0844093df",
   "metadata": {},
   "source": [
    "# Pad Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db0ac9e-2f4d-41aa-b02a-d4478fbc7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(pad):\n",
    "    # Convert to PyTorch tensor\n",
    "    pad_tensor = [torch.tensor(seq) for seq in pad]\n",
    "    \n",
    "    # Pad sequences using pad_sequence\n",
    "    padded_seq = pad_sequence(pad_tensor, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b73e2a1c-604a-44a1-8ea4-0b359d20af31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0, 24,  ...,  0,  0,  0],\n",
       "        [ 0,  0, 24,  ...,  0,  0,  0],\n",
       "        [ 0,  0, 24,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [24,  0,  0,  ...,  0,  0,  0],\n",
       "        [10, 24,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = pad_seq(eng_sequences)\n",
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "457f8bb9-6b76-4b80-983b-2f5a9475ac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 177])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6f5386-646e-4adb-9912-86880c53aee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 23,  8,  ...,  0,  0,  0],\n",
       "        [ 4, 23,  8,  ...,  0,  0,  0],\n",
       "        [ 4, 23,  8,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 4, 23,  8,  ...,  0,  0,  0],\n",
       "        [ 4, 23,  8,  ...,  0,  0,  0],\n",
       "        [ 4, 23,  8,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = pad_seq(port_inp)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70b3982f-42a4-4a77-a545-43f0e2e68c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 195])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "185dc8aa-e7e1-42c3-a92d-603337c29029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23,  8, 23,  ...,  0,  0,  0],\n",
       "        [23,  8, 23,  ...,  0,  0,  0],\n",
       "        [23,  8, 23,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [23,  8, 23,  ...,  0,  0,  0],\n",
       "        [23,  8, 23,  ...,  0,  0,  0],\n",
       "        [23,  8, 23,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output = pad_seq(port_out)\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13af1533-8896-4b5f-85f5-3cf0ecb43a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 195])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85106181-7db9-45ba-9eaf-97eb42990a91",
   "metadata": {},
   "source": [
    "# Training the PyTorch Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff53bfcc-4656-470b-84e1-d540fed4f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n",
      "Model parameters device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "src_vocab_size = eng_vocab\n",
    "tgt_vocab_size = port_vocab\n",
    "batch_size = 64 #prev 32\n",
    "d_model = 512 #256 #prev 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 1024 #512 #1024 #prev 2048\n",
    "max_seq_len = max(encoder_input.size(1), decoder_input.size(1))\n",
    "dropout = 0.1\n",
    "epoch = None\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Current device: {device}\")\n",
    "\n",
    "# Sequence Length\n",
    "src_seq_len = encoder_input.shape[1]\n",
    "tgt_seq_len = decoder_input.shape[1]\n",
    "\n",
    "# Src and tgt datasets\n",
    "src_data = encoder_input #torch.tensor(df['EN'])\n",
    "tgt_data = decoder_input #torch.tensor(df['PT'])\n",
    "\n",
    "# Create an instance of the Transformer model\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "\n",
    "# Move model and data to the selected device\n",
    "transformer.to(device)\n",
    "#src_data, tgt_data = src_data.to(device), tgt_data.to(device)\n",
    "\n",
    "# Print the device on which the model parameters are located\n",
    "print(f\"Model parameters device: {next(transformer.parameters()).device}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Create ReduceLROnPlateau scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "# Scaler\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f6455-f471-4ae3-9f93-206055c40c1e",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Splits, and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2052c4c0-0fef-49aa-a456-15a5a6859012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_src_data, val_src_data, train_tgt_data, val_tgt_data = train_test_split(\n",
    "    src_data, tgt_data, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fed643f-d764-4e9c-baeb-550a780f4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a custom dataset class, let's call it MyDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sample = self.src_data[idx].to(device)\n",
    "        tgt_sample = self.tgt_data[idx].to(device)\n",
    "        return src_sample, tgt_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe2a1b41-1478-48c9-a1e5-fb1e51dd2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your dataset\n",
    "train_dataset = MyDataset(train_src_data, train_tgt_data)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "927bf1b1-c6e4-4d15-b550-8d4c3930a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your validation dataset\n",
    "val_dataset = MyDataset(val_src_data, val_tgt_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ad3ce-3528-4e70-a80e-8f68ba648168",
   "metadata": {},
   "source": [
    "## Compute Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cbe8d5a-854d-49e6-8825-a764b465b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(output, target):\n",
    "    \"\"\"\n",
    "    Compute accuracy based on the model's output and target values.\n",
    "\n",
    "    Args:\n",
    "    - output (torch.Tensor): Model's output tensor.\n",
    "    - target (torch.Tensor): Target tensor.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Accuracy value.\n",
    "    \"\"\"\n",
    "    # Assuming output and target are tensors\n",
    "    # Get the predicted indices with maximum probability\n",
    "    _, predicted = torch.max(output, dim=2)\n",
    "\n",
    "    # Mask for valid positions (non-padding)\n",
    "    mask = (target != 0).float()\n",
    "\n",
    "    # Compare predicted and target values, count correct predictions\n",
    "    correct = (predicted == target) * mask\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct.sum().item() / mask.sum().item() if mask.sum().item() > 0 else 0.0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70127be-36e6-4851-8844-8f5efc96be58",
   "metadata": {},
   "source": [
    "## Create/Load Checkpoints and Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef866a09-5f22-4c6e-a54c-acac06b96797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# File path for the saved checkpoint\n",
    "checkpoint_file_path = './checkpoint.pth'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    # Load the saved checkpoint\n",
    "    checkpoint = torch.load(checkpoint_file_path)\n",
    "\n",
    "    # Load epoch\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    # Load transformer\n",
    "    transformer_checkpoint = checkpoint['model_state_dict']\n",
    "    keys_to_load = ['encoder_embedding', 'decoder_embedding', 'positional_encoding', \n",
    "                    'encoder_layers', 'decoder_layers', 'fc']\n",
    "    filtered_state_dict = {k: v for k, v in transformer_checkpoint.items() if k in keys_to_load}\n",
    "    transformer.load_state_dict(filtered_state_dict, strict=False)\n",
    "    #transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load the optimizer state dict\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load Scheduler\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    # Load Scaler\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "else:\n",
    "    # If the file doesn't exist, create and save checkpoint\n",
    "    epoch = 0\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'scaler_state_dict': scaler.state_dict(),\n",
    "    }, checkpoint_file_path)\n",
    "    print(\"Checkpoint file not found. Initial checkpoint created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9de7c92-1df2-4837-9d5d-774bb17f0938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress loaded successfully\n",
      "Information in the training progress:\n",
      "Epoch: 1, Avg Loss: 1.6350, Avg Accuracy: 0.5410, Avg Val Loss: 1.1555, Avg Val Accuracy: 0.6443, Learning Rate: 0.0001\n",
      "Epoch: 2, Avg Loss: 1.0160, Avg Accuracy: 0.6878, Avg Val Loss: 0.9298, Avg Val Accuracy: 0.7145, Learning Rate: 0.0001\n",
      "Epoch: 3, Avg Loss: 0.8640, Avg Accuracy: 0.7341, Avg Val Loss: 0.8278, Avg Val Accuracy: 0.7454, Learning Rate: 0.0001\n",
      "Epoch: 4, Avg Loss: 0.7765, Avg Accuracy: 0.7604, Avg Val Loss: 0.7659, Avg Val Accuracy: 0.7643, Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# File path for the saved checkpoint\n",
    "progress_file_path = './training_progress.pth'\n",
    "training_progress = None\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(progress_file_path):\n",
    "    # Load the saved checkpoint\n",
    "    print(\"Training progress loaded successfully\")\n",
    "    training_progress = torch.load(progress_file_path)\n",
    "\n",
    "    print(\"Information in the training progress:\")\n",
    "    for epoch_info in training_progress:\n",
    "        print(f\"Epoch: {epoch_info['epoch']}, \"\n",
    "              f\"Avg Loss: {epoch_info['avg_loss']:.4f}, \"\n",
    "              f\"Avg Accuracy: {epoch_info['avg_accuracy']:.4f}, \"\n",
    "              f\"Avg Val Loss: {epoch_info['avg_val_loss']:.4f}, \"\n",
    "              f\"Avg Val Accuracy: {epoch_info['avg_val_accuracy']:.4f}, \"\n",
    "              f\"Learning Rate: {epoch_info['learning_rate']:.4f}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    # If the file doesn't exist, create and save training progress\n",
    "    training_progress = []\n",
    "    torch.save(training_progress, progress_file_path)\n",
    "    print(\"Training progress file not found. File created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50f181cd-befb-419d-9d81-84b220c1e09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(10783, 512)\n",
       "  (decoder_embedding): Embedding(16450, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=16450, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b510939b-10ae-4a58-ae7a-2525d4e3f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a quantile normalization function\n",
    "# def quantile_normalize(output):\n",
    "#     # Convert outputs to NumPy array\n",
    "#     output_np = output.detach().cpu().numpy()\n",
    "\n",
    "#     # Calculate quantiles\n",
    "#     quantiles = np.mean(np.sort(output_np, axis=0), axis=1)\n",
    "\n",
    "#     # Apply quantile normalization\n",
    "#     normalized_output_np = np.interp(output_np, quantiles, np.linspace(0, 1, len(quantiles)))\n",
    "\n",
    "#     # Convert back to PyTorch tensor\n",
    "#     normalized_output = torch.from_numpy(normalized_output_np).to(output.device)\n",
    "\n",
    "#     return normalized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec9df3c0-dfef-495b-83e4-a279f63597aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quantile_normalize(output):\n",
    "#     # Convert PyTorch tensor to NumPy array\n",
    "#     output_np = output.cpu().detach().numpy()\n",
    "\n",
    "#     # Flatten the NumPy array\n",
    "#     flat_output_np = output_np.flatten()\n",
    "\n",
    "#     # Compute quantiles\n",
    "#     quantiles = np.mean(np.sort(flat_output_np))\n",
    "\n",
    "#     # Apply quantile normalization to flattened array\n",
    "#     normalized_flat_output_np = np.interp(flat_output_np, quantiles, np.linspace(0, 1, len(quantiles)))\n",
    "\n",
    "#     # Reshape the normalized array to the original shape\n",
    "#     normalized_output_np = normalized_flat_output_np.reshape(output_np.shape)\n",
    "\n",
    "#     # Convert back to PyTorch tensor\n",
    "#     normalized_output = torch.from_numpy(normalized_output_np).to(output.device)\n",
    "\n",
    "#     return normalized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4e1dfbb-215b-43b7-939d-3ab606760f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quantile_normalize(output):\n",
    "#     # Convert PyTorch tensor to NumPy array\n",
    "#     output_np = output.cpu().detach().numpy()\n",
    "\n",
    "#     # Flatten the NumPy array\n",
    "#     flat_output_np = output_np.flatten()\n",
    "\n",
    "#     # Compute quantiles\n",
    "#     quantiles = np.linspace(0, 1, len(flat_output_np))\n",
    "\n",
    "#     # Apply quantile normalization to flattened array\n",
    "#     normalized_flat_output_np = np.interp(flat_output_np, np.sort(flat_output_np), quantiles)\n",
    "\n",
    "#     # Reshape the normalized array to the original shape\n",
    "#     normalized_output_np = normalized_flat_output_np.reshape(output_np.shape)\n",
    "\n",
    "#     # Convert back to PyTorch tensor\n",
    "#     normalized_output = torch.from_numpy(normalized_output_np).to(output.device)\n",
    "\n",
    "#     return normalized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1883d1a-e055-4b05-a6f0-2cc9fc52fd90",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5636b873-5260-47bc-9367-22c7c92eb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Enable anomaly detection\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bccf48b2-742d-47fe-97b4-1d5200c66699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88da387339294c7baf1a6448a84e2989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/100, Training Set:   0%|                                                                              …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 31\u001b[0m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_vocab_size), train_tgt_batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# # Normalize the model outputs using quantile normalization\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# normalized_output = quantile_normalize(output)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Backward Pass\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Gradient Clipping\u001b[39;00m\n\u001b[0;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(transformer\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\GPT_translator\\cuda\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\GPT_translator\\cuda\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "transformer.train()\n",
    "\n",
    "for current_epoch in range(epoch, 100):\n",
    "    epoch = current_epoch\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_accuracy = 0.0\n",
    "    \n",
    "    print(f\"Starting Epoch: {epoch+1}\")\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run \n",
    "    for train_batch_idx, (train_src_batch, train_tgt_batch) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/100, Training Set\", position=0, ncols=240), start=1):\n",
    "#    for train_batch_idx, (train_src_batch, train_tgt_batch) in enumerate(train_dataloader, start=1):\n",
    "        with autocast():\n",
    "            # Forward Pass\n",
    "            output = transformer(train_src_batch, train_tgt_batch[:, :-1])\n",
    "\n",
    "            # Compute the loss using normalized outputs\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), train_tgt_batch[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            # # Normalize the model outputs using quantile normalization\n",
    "            # normalized_output = quantile_normalize(output)\n",
    "\n",
    "            # # Compute the loss using normalized outputs\n",
    "            # loss = criterion(normalized_output.contiguous().view(-1, tgt_vocab_size), train_tgt_batch[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        # Backward Pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1)\n",
    "\n",
    "        # Update Optimizer\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate total validation loss\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        train_accuracy = compute_accuracy(output, train_tgt_batch[:, 1:])\n",
    "        total_accuracy += train_accuracy\n",
    "        avg_accuracy = total_accuracy / len(train_dataloader)\n",
    "        \n",
    "        # Print progress\n",
    "        if train_batch_idx % 50 == 0:  # Print every 25 batches\n",
    "            print(f\"Epoch: {epoch+1}/100, Train Batch: {train_batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Train_Batch for Epoch {epoch+1} completed. Avg_Loss: {avg_loss:.4f}, Avg_Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    for val_batch_idx, (val_src_batch, val_tgt_batch) in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/100, Validation Set\", position=0, ncols=240), start=1):\n",
    "#    for val_batch_idx, (val_src_batch, val_tgt_batch) in enumerate(val_dataloader, start=1):\n",
    "        with torch.no_grad():\n",
    "            val_output = transformer(val_src_batch, val_tgt_batch[:, :-1])\n",
    "            val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_batch[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        val_accuracy = compute_accuracy(val_output, val_tgt_batch[:, 1:])\n",
    "        total_val_accuracy += val_accuracy\n",
    "        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n",
    "        \n",
    "        # Print progress\n",
    "        if val_batch_idx % 50 == 0:  # Print every 25 batches (adjust as needed)\n",
    "            print(f\"Epoch: {epoch+1}/100, Val_Batch: {val_batch_idx}/{len(val_dataloader)}, Val_Loss: {val_loss.item():.4f}, Val_Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Val_Batch for Epoch {epoch+1} completed. Avg_Val_Loss: {avg_val_loss:.4f}, Avg_Val_Accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    # Update scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "    }, checkpoint_file_path)\n",
    "    print(f\"Model, Optimizer, Scheduler, and Scaler states for Epoch {epoch+1} updated and saved.\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete, Avg_Loss: {avg_loss:.4f}, Avg_Accuracy: {avg_accuracy:.4f}, Avg_Val_Loss: {avg_val_loss:.4f}, Avg_Val_Accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save training progress (average loss and average accuracy)\n",
    "    training_progress.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'avg_loss': avg_loss,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'avg_val_loss': avg_val_loss,\n",
    "        'avg_val_accuracy': avg_val_accuracy,\n",
    "        'learning_rate': current_learning_rate\n",
    "    })\n",
    "\n",
    "    print(f\"Learning Rate: {current_learning_rate:.8f}\")\n",
    "\n",
    "    # Save the updated training progress\n",
    "    torch.save(training_progress, progress_file_path)\n",
    "    print(f\"Training progress updated and saved.\")\n",
    "\n",
    "    # Check for early stopping using scheduler metrics\n",
    "    if scheduler.num_bad_epochs > scheduler.patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c278d6-8ac4-4013-bc51-bf0dab420ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disable anomaly detection after training\n",
    "# torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7bf31-fb90-405a-8f52-2f7425ca95d5",
   "metadata": {},
   "source": [
    "# Plot a Graph with the info of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da569e-9def-433c-8cd4-347368eaa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the training progresss\n",
    "\n",
    "#Loss, Accuracy, Val_loss, Val_accuracy\n",
    "#Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79443976-1af5-4030-9f45-c991cefdeb70",
   "metadata": {},
   "source": [
    "# Prepare Inputs, Decode, and Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89659b3e-2b99-4c7b-b188-66eff3a5b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(sentences):\n",
    "    input_tokenized = eng_field.numericalize([sentences])\n",
    "    input_padded = pad_sequence(input_tokenized, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Truncate or zero-pad to the specified maxlen\n",
    "    if input_padded.size(1) > src_seq_len:\n",
    "        input_padded = input_padded[:, :src_seq_len]\n",
    "    else:\n",
    "        input_padded = F.pad(input_padded, (0, src_seq_len - input_padded.size(1)))\n",
    "    \n",
    "    input_tensor = torch.tensor(input_padded, dtype=torch.long)\n",
    "    \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1788f-ae4e-4039-b4c8-0efc60c865fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #NEW VERSION TO TEST OUT\n",
    "\n",
    "# def prepare_inputs(sentences):\n",
    "#     sentences = eng_preprocess(sentences)\n",
    "    \n",
    "#     # Step 1: Create a tokenizer\n",
    "#     tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "#     # Step 2: Create a torchtext Field for English features\n",
    "#     input_field = Field(tokenize=tokenizer, lower=True)\n",
    "\n",
    "#     # Step 3: Use the Field to process the feature text\n",
    "#     input_field.build_vocab([input_field.preprocess(word) for word in sentences])\n",
    "\n",
    "#     # Step 4: Convert feature text to sequences\n",
    "#     input_seq = [input_field.numericalize([word]) for word in sentences]\n",
    "    \n",
    "#     # # Step 5: Convert PyTorch tensors to Python lists\n",
    "#     # input_seq = [seq.squeeze().tolist() for seq in input_seq]\n",
    "\n",
    "#     # Step 5: Pad or truncate sequences using pad_sequence\n",
    "#     input_padded = pad_sequence(input_seq, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     # Truncate or zero-pad to the specified maxlen\n",
    "#     if input_padded.size(1) > src_seq_len:\n",
    "#         input_padded = input_padded[:, :src_seq_len]\n",
    "#     else:\n",
    "#         input_padded = F.pad(input_padded, (0, src_seq_len - input_padded.size(1)))\n",
    "    \n",
    "#     input_tensor = torch.tensor(input_padded, dtype=torch.long)\n",
    "\n",
    "#     return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0c4ca-18b3-4b0e-9d7c-874ad24a5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_process(d):\n",
    "    target_tokenized = port_field.numericalize([d])\n",
    "    target_padded = pad_sequence(target_tokenized, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Truncate or zero-pad to the specified maxlen\n",
    "    if target_padded.size(1) > tgt_seq_len:\n",
    "        target_padded = target_padded[:, :tgt_seq_len]\n",
    "    else:\n",
    "        target_padded = F.pad(target_padded, (0, tgt_seq_len - target_padded.size(1)))\n",
    "    \n",
    "    dt = torch.tensor(target_padded, dtype=torch.long)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc1ce4-5cf2-4563-b5d1-3d4e1887d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_end = [\"<EOS>\"]\n",
    "end = port_field.numericalize([decoder_end])\n",
    "end = end[0].clone().detach()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2206f-2d26-462e-8719-0e070adfc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inputs):\n",
    "    # Set the model to evaluation mode\n",
    "    transformer.eval()\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    tt = prepare_inputs(inputs)\n",
    "    \n",
    "    # Initialize the target input with \"<SOS>\"\n",
    "    d = [\"<sos>\"]\n",
    "\n",
    "    # Numericalize the target input\n",
    "    dt = decode_process(d)\n",
    "\n",
    "    # Move tensors to the appropriate device (GPU if available)\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tt = tt.to(device)\n",
    "    dt = dt.to(device)\n",
    "    \n",
    "    # Generate translations\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(tgt_seq_len), desc=\"Generating Translations\", position=0, ncols=240):\n",
    "            # Forward pass\n",
    "            output = transformer(tt, dt)\n",
    "\n",
    "            # Get the indices of the predicted tokens at the last position\n",
    "            pred_indices = torch.argmax(output[:, -1, :], dim=-1)\n",
    "\n",
    "            # Extract the predicted tokens as a list\n",
    "            pred_tokens = [port_field.vocab.itos[index.item()] for index in pred_indices]\n",
    "\n",
    "            # Append the predicted tokens to the target sequence\n",
    "            d.extend(pred_tokens)\n",
    "\n",
    "            # # Stop if <EOS> is predicted\n",
    "            # if (pred_indices == port_field.vocab.stoi[\"<eos>\"]).any():\n",
    "            #     break\n",
    "\n",
    "            # Stop if <EOS> is predicted\n",
    "            if (pred_indices == port_field.vocab.stoi[end]).any():\n",
    "                break\n",
    "            \n",
    "            # Update the target input tensor for the next step\n",
    "            dt = torch.cat([dt, pred_indices.unsqueeze(0)], dim=-1)\n",
    "            \n",
    "            # Enforce max_seq_length by keeping the last max_seq_length elements\n",
    "            dt = dt[:, -max_seq_len:]\n",
    "\n",
    "    # Convert the generated translation back to a string\n",
    "    translation = \" \".join(d)\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95313b5f-1fd6-45a1-9ba0-12af4cee0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"English Sentence : \", df['EN'][i])\n",
    "    print(\"Original Portugal Sentence : \", df['PT'][i])\n",
    "    print(\"Predicted Sentence : \", translate(df[\"EN\"][i]))\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6744f43-1d3d-4d4d-9173-f9b11e4db562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a73e6f-f1bf-4b10-93bc-7a4b413f6632",
   "metadata": {},
   "source": [
    "# Test Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea4a92-b8a6-47f2-bbfe-82686598e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"The weather is beautiful today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19dbdd-2cac-47f2-8749-2ffbf6762228",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I enjoy reading books in my free time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a811c-62c7-4973-92d4-f31a7c7e53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Here is the nearest coffee shop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514499e-83c4-4ea4-b88e-45a5b00f85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Learning new languages broadens your perspective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60694e29-7f33-4f8a-ab1d-d9105c0addbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Can you recommend a good restaurant in the area?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277e9c2-47fd-40b7-a30d-6705788c774a",
   "metadata": {},
   "source": [
    "# Input Text and Get Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d5f0b-f3e9-461f-9967-c370a3fceb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_input():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Enter an English sentence (type 'exit' to quit): \")\n",
    "\n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting translation program.\")\n",
    "            break\n",
    "\n",
    "        # Use your existing translate function\n",
    "        translation = translate(user_input)\n",
    "\n",
    "        # Display the translation\n",
    "        print(f\"Translation: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8759d-e144-4360-b205-c46c5ae02fde",
   "metadata": {},
   "source": [
    "# Call The Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff50a1-05ba-48c0-ab2c-ac7031bcacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79696dac-1074-4aa2-b1bb-6c45a91f8db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a604b32-6f15-4a86-aada-180200937d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Memory Allocated: 1.516465664 GB\n",
      "Initial Max GPU Memory Allocated: 8.536550912 GB\n",
      "Final GPU Memory Allocated: 1.516465664 GB\n",
      "Final Max GPU Memory Allocated: 8.536550912 GB\n"
     ]
    }
   ],
   "source": [
    "# Get initial GPU memory usage\n",
    "initial_memory_allocated = torch.cuda.memory_allocated()\n",
    "initial_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# ... your code ...\n",
    "\n",
    "# Run torch.cuda.empty_cache() to release unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Get GPU memory usage after running empty_cache()\n",
    "final_memory_allocated = torch.cuda.memory_allocated()\n",
    "final_max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Initial GPU Memory Allocated: {initial_memory_allocated / 1e9} GB\")\n",
    "print(f\"Initial Max GPU Memory Allocated: {initial_max_memory_allocated / 1e9} GB\")\n",
    "\n",
    "print(f\"Final GPU Memory Allocated: {final_memory_allocated / 1e9} GB\")\n",
    "print(f\"Final Max GPU Memory Allocated: {final_max_memory_allocated / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f863c70-f580-4be0-8624-71d5bc911d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_GPT_translator",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
