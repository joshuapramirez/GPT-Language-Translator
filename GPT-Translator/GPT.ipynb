{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb10b915-e513-40d4-852d-0b087d987aa0",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2rem;color:green;\">English to Portuguese Translator LLM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bd07-18ce-4aca-9214-fe217ca94335",
   "metadata": {},
   "source": [
    "## Set up PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33695f12-9591-4444-a964-3c1132c3d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas\n",
    "!pip install ipywidgets\n",
    "!pip install widgetsnbextension\n",
    "!pip install jupyter_contrib_nbextensions\n",
    "!jupyter contrib nbextension install --user\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2186ad4-7342-4ec6-b168-97a93b66af7f",
   "metadata": {},
   "source": [
    "## Import the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b80005-deb8-4b0b-9174-b4832e9e5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689cd82-b256-4d28-b9a2-18fca3c8c3b8",
   "metadata": {},
   "source": [
    "# Defining the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116b6d4-9cab-42e5-a5e8-9ea303cb955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            _MASKING_VALUE = -1e+30 if attn_scores.dtype == torch.float32 else -1e+4            \n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, value=_MASKING_VALUE)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a03d9a-3674-4a5f-b02d-9cb21b7e89a0",
   "metadata": {},
   "source": [
    "## 1. Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96f8ee-42a2-4ece-852d-42f7988dc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81f4f9-a0a1-4b94-a121-7dbb1c2dbac0",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab59428-74c5-49af-a45a-214b5601c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length  # Store max_seq_length explicitly\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f'x size: {x.size()}')\n",
    "        # print(f'self.pe size: {self.pe.size()}')\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45275168-5ae2-41c3-98e7-2e966d126851",
   "metadata": {},
   "source": [
    "## 3. Building the Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86792e14-9ce9-4392-b3fb-ef30c4b1d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d2041-cc0e-45fb-bfc9-93d5a548c74b",
   "metadata": {},
   "source": [
    "## 4. Building the Decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c711-0279-4216-9c2b-019c88045e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc0d17-3d30-4a4f-a6d2-2469b72be26c",
   "metadata": {},
   "source": [
    "## 5. Combine the Encoder and Decoder layers to complete the Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75509b7c-d556-4fb3-aa97-cc6d2ae9aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        # Ensure tgt_mask is on the same device as tgt\n",
    "        #tgt_mask = tgt_mask.to(tgt.device)\n",
    "\n",
    "        nopeak_mask = nopeak_mask.to(tgt.device)\n",
    "        \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950fe9c-a625-4cf0-98cd-9c1b0496ad2f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:2rem;color:green;\">Dataset Loading and Tokenization, and Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdcdf7-6ec4-4be2-b152-456a2f9b4427",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d8880-fd5f-45a6-a523-cabd21e5bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext==0.6.0 spacy numpy nltk scikit-learn tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da134f-b4f3-4529-b8c0-6b8570cbb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer, Field, TabularDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d58c5-a03d-4597-9e62-f8e625dd7275",
   "metadata": {},
   "source": [
    "# Load English to Portuguese Translation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eba007-4cf8-44a8-ba48-f91200fbec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English to Portuguese translation data\n",
    "dataset_file_path = './por.txt'  # Adjust the path accordingly\n",
    "\n",
    "# Load and preprocess translation data\n",
    "df = pd.read_csv(dataset_file_path, sep='\\t', header=None)[[0, 1]].rename(columns={0: 'EN', 1: 'PT'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09e0db-d466-41aa-b39a-21f50ab1bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734877-7322-4b74-8100-50ef9ffc28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the rows and reindex\n",
    "df = df.sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf73afb-6268-4b80-88ba-c85077c2fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100000]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386ae48-c4d3-4279-9b57-dfc8be7fc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Extra Context for Tranformer\n",
    "df['PT'] = df['PT'].apply(lambda x: \"<SOS> \" + x + \" <EOS>\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4bc8b-436e-48e2-9692-03d959ae175b",
   "metadata": {},
   "source": [
    "# Preprocess/Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db7008-09be-4ef9-882d-eee788b8d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(re.compile(r'[^a-zA-Z0-9\\s]'), '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = \" \".join([i.strip() for i in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def pt_preprocess(text):\n",
    "    text = text.replace(\"\\u202f\",\" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub(re.compile(\"[^a-zéâàçêêëôîû!?',]\"), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cafba9-9648-4068-9a62-04fbfb8f1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EN'] = df['EN'].apply(lambda x: eng_preprocess(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20761db9-4273-47ee-8bda-3ab856b6fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PT'] = df['PT'].apply(lambda x: pt_preprocess(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d51b47-0b9a-401e-ae77-f11a6804a64c",
   "metadata": {},
   "source": [
    "# Tokenize Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d2af8-b180-4a99-9079-56692a650b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tokenization(feat):\n",
    "    # Step 1: Create a tokenizer\n",
    "    tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "    # Step 2: Create a torchtext Field for English features\n",
    "    eng_field = Field(tokenize=tokenizer, lower=True)\n",
    "\n",
    "    # Step 3: Use the Field to process the feature text\n",
    "    eng_field.build_vocab([eng_field.preprocess(f) for f in feat])\n",
    "\n",
    "    # Step 4: Convert feature text to sequences\n",
    "    eng_seq = [eng_field.numericalize([f]) for f in feat]\n",
    "\n",
    "    # Step 5: Convert PyTorch tensors to Python lists\n",
    "    eng_seq = [seq.squeeze().tolist() for seq in eng_seq]\n",
    "\n",
    "    return eng_seq, eng_field\n",
    "\n",
    "def target_tokenization(target):\n",
    "    # Step 1: Create a tokenizer\n",
    "    tokenizer = get_tokenizer(\"spacy\", language=\"pt_core_news_sm\")\n",
    "\n",
    "    # Step 2: Create a torchtext Field for Portuguese targets\n",
    "    port_field = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "    # Step 3: Use the Field to process the target text\n",
    "    port_field.build_vocab([port_field.preprocess(t) for t in target])\n",
    "\n",
    "    # Step 4: Convert target text to sequences\n",
    "    port_seq = [port_field.numericalize([t]) for t in target]\n",
    "\n",
    "    # Step 5: Convert PyTorch tensors to Python lists\n",
    "    port_seq = [seq.squeeze().tolist() for seq in port_seq]\n",
    "\n",
    "    return port_seq, port_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ad18a-39a3-4626-bcd5-10bd3a9dbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sequences, eng_field = feature_tokenization(df['EN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad545b8e-aaf1-4151-97d4-feeea14c4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = len(eng_field.vocab) + 1\n",
    "eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399e7a9-c5ae-4ad0-b037-eff8db83c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadfb6b-71fb-4bab-82eb-a6db4a69ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_sequences, port_field = target_tokenization(df['PT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d1051-b56d-4320-8d29-e4865b3d76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_vocab = len(port_field.vocab) + 1\n",
    "port_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22b41-02d2-40f8-89e7-6b91a8a1058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71755b5e-f40a-4132-8785-3f26575314f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_inp = [x[:-1] for x in port_sequences]\n",
    "port_inp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab112cea-74f7-4598-b67d-e0357ac22b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_out = [x[1:] for x in port_sequences]\n",
    "port_out[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cee15-64b6-4e48-b332-65e0844093df",
   "metadata": {},
   "source": [
    "# Pad Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0ac9e-2f4d-41aa-b02a-d4478fbc7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(pad):\n",
    "    # Convert to PyTorch tensor\n",
    "    pad_tensor = [torch.tensor(seq) for seq in pad]\n",
    "    \n",
    "    # Pad sequences using pad_sequence\n",
    "    padded_seq = pad_sequence(pad_tensor, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e2a1c-604a-44a1-8ea4-0b359d20af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_seq(eng_sequences)\n",
    "encoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f8bb9-6b76-4b80-983b-2f5a9475ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f5386-646e-4adb-9912-86880c53aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = pad_seq(port_inp)\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3982f-42a4-4a77-a545-43f0e2e68c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dc8aa-e7e1-42c3-a92d-603337c29029",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = pad_seq(port_out)\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af1533-8896-4b5f-85f5-3cf0ecb43a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85106181-7db9-45ba-9eaf-97eb42990a91",
   "metadata": {},
   "source": [
    "# Training the PyTorch Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53bfcc-4656-470b-84e1-d540fed4f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "src_vocab_size = eng_vocab\n",
    "tgt_vocab_size = port_vocab\n",
    "batch_size = 128\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 1024 #prev 2048\n",
    "max_seq_len = max(encoder_input.size(1), decoder_input.size(1))\n",
    "dropout = 0.1\n",
    "epoch = None\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Current device: {device}\")\n",
    "\n",
    "# Sequence Length\n",
    "src_seq_len = encoder_input.shape[1]\n",
    "tgt_seq_len = decoder_input.shape[1]\n",
    "\n",
    "# Src and tgt datasets\n",
    "src_data = encoder_input\n",
    "tgt_data = decoder_input\n",
    "\n",
    "# Create an instance of the Transformer model\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "\n",
    "# Move model and data to the selected device\n",
    "transformer.to(device)\n",
    "\n",
    "# Print the device on which the model parameters are located\n",
    "print(f\"Model parameters device: {next(transformer.parameters()).device}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Create ReduceLROnPlateau scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
    "\n",
    "# Scaler\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f6455-f471-4ae3-9f93-206055c40c1e",
   "metadata": {},
   "source": [
    "## Prepare Training and Validation Splits, and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052c4c0-0fef-49aa-a456-15a5a6859012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_src_data, val_src_data, train_tgt_data, val_tgt_data = train_test_split(\n",
    "    src_data, tgt_data, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed643f-d764-4e9c-baeb-550a780f4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a custom dataset class, let's call it MyDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sample = self.src_data[idx].to(device)\n",
    "        tgt_sample = self.tgt_data[idx].to(device)\n",
    "        return src_sample, tgt_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a1b41-1478-48c9-a1e5-fb1e51dd2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your dataset\n",
    "train_dataset = MyDataset(train_src_data, train_tgt_data)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bf1b1-c6e4-4d15-b550-8d4c3930a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your validation dataset\n",
    "val_dataset = MyDataset(val_src_data, val_tgt_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ad3ce-3528-4e70-a80e-8f68ba648168",
   "metadata": {},
   "source": [
    "## Compute Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8d5a-854d-49e6-8825-a764b465b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(output, target):\n",
    "    \"\"\"\n",
    "    Compute accuracy based on the model's output and target values.\n",
    "\n",
    "    Args:\n",
    "    - output (torch.Tensor): Model's output tensor.\n",
    "    - target (torch.Tensor): Target tensor.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy (float): Accuracy value.\n",
    "    \"\"\"\n",
    "    # Assuming output and target are tensors\n",
    "    # Get the predicted indices with maximum probability\n",
    "    _, predicted = torch.max(output, dim=2)\n",
    "\n",
    "    # Mask for valid positions (non-padding)\n",
    "    mask = (target != 0).float()\n",
    "\n",
    "    # Compare predicted and target values, count correct predictions\n",
    "    correct = (predicted == target) * mask\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct.sum().item() / mask.sum().item() if mask.sum().item() > 0 else 0.0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70127be-36e6-4851-8844-8f5efc96be58",
   "metadata": {},
   "source": [
    "## Create/Load Checkpoints and Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef866a09-5f22-4c6e-a54c-acac06b96797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File path for the saved checkpoint\n",
    "checkpoint_file_path = './checkpoint.pth'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    # Load the saved checkpoint\n",
    "    checkpoint = torch.load(checkpoint_file_path)\n",
    "\n",
    "    # Load epoch\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    # Load transformer\n",
    "    transformer_checkpoint = checkpoint['model_state_dict']\n",
    "    keys_to_load = ['encoder_embedding', 'decoder_embedding', 'positional_encoding', \n",
    "                    'encoder_layers', 'decoder_layers', 'fc']\n",
    "    filtered_state_dict = {k: v for k, v in transformer_checkpoint.items() if k in keys_to_load}\n",
    "    transformer.load_state_dict(filtered_state_dict, strict=False)\n",
    "    #transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load the optimizer state dict\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load Scheduler\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    # Load Scaler\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "else:\n",
    "    # If the file doesn't exist, create and save checkpoint\n",
    "    epoch = 0\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'scaler_state_dict': scaler.state_dict(),\n",
    "    }, checkpoint_file_path)\n",
    "    print(\"Checkpoint file not found. Initial checkpoint created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de7c92-1df2-4837-9d5d-774bb17f0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the saved checkpoint\n",
    "progress_file_path = './training_progress.pth'\n",
    "training_progress = None\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(progress_file_path):\n",
    "    # Load the saved checkpoint\n",
    "    print(\"Training progress loaded successfully\")\n",
    "    training_progress = torch.load(progress_file_path)\n",
    "\n",
    "    print(\"Information in the training progress:\")\n",
    "    for epoch_info in training_progress:\n",
    "        print(f\"Epoch: {epoch_info['epoch']}, \"\n",
    "              f\"Avg Loss: {epoch_info['avg_loss']:.4f}, \"\n",
    "              f\"Avg Accuracy: {epoch_info['avg_accuracy']:.4f}, \"\n",
    "              f\"Avg Val Loss: {epoch_info['avg_val_loss']:.4f}, \"\n",
    "              f\"Avg Val Accuracy: {epoch_info['avg_val_accuracy']:.4f}, \"\n",
    "              f\"Learning Rate: {epoch_info['learning_rate']:.4f}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    # If the file doesn't exist, create and save training progress\n",
    "    training_progress = []\n",
    "    torch.save(training_progress, progress_file_path)\n",
    "    print(\"Training progress file not found. File created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f181cd-befb-419d-9d81-84b220c1e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151aace2-49e8-4c17-974b-4febec3cbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1883d1a-e055-4b05-a6f0-2cc9fc52fd90",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf48b2-742d-47fe-97b4-1d5200c66699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "transformer.train()\n",
    "\n",
    "for current_epoch in range(epoch, 100):\n",
    "    epoch = current_epoch\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_accuracy = 0.0\n",
    "    \n",
    "    print(f\"Starting Epoch: {epoch+1}\")\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Run \n",
    "    for train_batch_idx, (train_src_batch, train_tgt_batch) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/100, Training Set\", position=0, ncols=240), start=1):\n",
    "        with autocast():\n",
    "            # Forward Pass\n",
    "            output = transformer(train_src_batch, train_tgt_batch[:, :-1])\n",
    "\n",
    "            # Compute the loss using normalized outputs\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), train_tgt_batch[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        # Backward Pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "#        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient Norm Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "        # Update Optimizer\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate total validation loss\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        train_accuracy = compute_accuracy(output, train_tgt_batch[:, 1:])\n",
    "        total_accuracy += train_accuracy\n",
    "        avg_accuracy = total_accuracy / len(train_dataloader)\n",
    "        \n",
    "        # Print progress\n",
    "        if train_batch_idx % 25 == 0:  # Print every 25 batches\n",
    "            print(f\"Epoch: {epoch+1}/100, Train Batch: {train_batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Train_Batch for Epoch {epoch+1} completed. Avg_Loss: {avg_loss:.4f}, Avg_Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    for val_batch_idx, (val_src_batch, val_tgt_batch) in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/100, Validation Set\", position=0, ncols=240), start=1):\n",
    "        with torch.no_grad():\n",
    "            val_output = transformer(val_src_batch, val_tgt_batch[:, :-1])\n",
    "            val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_batch[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        val_accuracy = compute_accuracy(val_output, val_tgt_batch[:, 1:])\n",
    "        total_val_accuracy += val_accuracy\n",
    "        avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n",
    "        \n",
    "        # Print progress\n",
    "        if val_batch_idx % 25 == 0:  # Print every 25 batches (adjust as needed)\n",
    "            print(f\"Epoch: {epoch+1}/100, Val_Batch: {val_batch_idx}/{len(val_dataloader)}, Val_Loss: {val_loss.item():.4f}, Val_Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Val_Batch for Epoch {epoch+1} completed. Avg_Val_Loss: {avg_val_loss:.4f}, Avg_Val_Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Update scheduler based on validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "    }, checkpoint_file_path)\n",
    "    print(f\"Model, Optimizer, Scheduler, and Scaler states for Epoch {epoch+1} updated and saved.\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete, Avg_Loss: {avg_loss:.4f}, Avg_Accuracy: {avg_accuracy:.4f}, Avg_Val_Loss: {avg_val_loss:.4f}, Avg_Val_Accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save training progress (average loss and average accuracy)\n",
    "    training_progress.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'avg_loss': avg_loss,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'avg_val_loss': avg_val_loss,\n",
    "        'avg_val_accuracy': avg_val_accuracy,\n",
    "        'learning_rate': current_learning_rate\n",
    "    })\n",
    "\n",
    "    print(f\"Learning Rate: {current_learning_rate:.8f}\")\n",
    "\n",
    "    # Save the updated training progress\n",
    "    torch.save(training_progress, progress_file_path)\n",
    "    print(f\"Training progress updated and saved.\")\n",
    "\n",
    "    # Check for early stopping using scheduler metrics\n",
    "    if scheduler.num_bad_epochs > scheduler.patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7bf31-fb90-405a-8f52-2f7425ca95d5",
   "metadata": {},
   "source": [
    "# Plot a Graph with the info of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da569e-9def-433c-8cd4-347368eaa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(training_progress):\n",
    "    epochs = [entry['epoch'] for entry in training_progress]\n",
    "    avg_loss = [entry['avg_loss'] for entry in training_progress]\n",
    "    avg_accuracy = [entry['avg_accuracy'] for entry in training_progress]\n",
    "    avg_val_loss = [entry['avg_val_loss'] for entry in training_progress]\n",
    "    avg_val_accuracy = [entry['avg_val_accuracy'] for entry in training_progress]\n",
    "    learning_rate = [entry['learning_rate'] for entry in training_progress]\n",
    "\n",
    "    # Plotting Loss\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, avg_loss, label='Training Loss')\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Accuracy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, avg_accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, avg_val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Learning Rate\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, learning_rate, label='Learning Rate')\n",
    "    plt.title('Learning Rate Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fdd0d-3fb4-4507-909f-16af2452b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot the training progress\n",
    "plot_training_progress(training_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79443976-1af5-4030-9f45-c991cefdeb70",
   "metadata": {},
   "source": [
    "# Prepare Inputs, Decode, and Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89659b3e-2b99-4c7b-b188-66eff3a5b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(sentences):\n",
    "    input_tokenized = eng_field.numericalize([sentences])\n",
    "    input_padded = pad_sequence(input_tokenized, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Truncate or zero-pad to the specified maxlen\n",
    "    if input_padded.size(1) > src_seq_len:\n",
    "        input_padded = input_padded[:, :src_seq_len]\n",
    "    else:\n",
    "        input_padded = F.pad(input_padded, (0, src_seq_len - input_padded.size(1)))\n",
    "    \n",
    "    input_tensor = torch.tensor(input_padded, dtype=torch.long)\n",
    "    \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0c4ca-18b3-4b0e-9d7c-874ad24a5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_process(d):\n",
    "    target_tokenized = port_field.numericalize([d])\n",
    "    target_padded = pad_sequence(target_tokenized, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Truncate or zero-pad to the specified maxlen\n",
    "    if target_padded.size(1) > tgt_seq_len:\n",
    "        target_padded = target_padded[:, :tgt_seq_len]\n",
    "    else:\n",
    "        target_padded = F.pad(target_padded, (0, tgt_seq_len - target_padded.size(1)))\n",
    "    \n",
    "    dt = torch.tensor(target_padded, dtype=torch.long)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc1ce4-5cf2-4563-b5d1-3d4e1887d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_end = [\"<EOS>\"]\n",
    "end = port_field.numericalize([decoder_end])\n",
    "end = end[0].clone().detach()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2206f-2d26-462e-8719-0e070adfc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inputs):\n",
    "    # Set the model to evaluation mode\n",
    "    transformer.eval()\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    tt = prepare_inputs(inputs)\n",
    "    \n",
    "    # Initialize the target input with \"<SOS>\"\n",
    "    d = [\"<sos>\"]\n",
    "\n",
    "    # Numericalize the target input\n",
    "    dt = decode_process(d)\n",
    "\n",
    "    # Move tensors to the appropriate device (GPU if available)\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tt = tt.to(device)\n",
    "    dt = dt.to(device)\n",
    "    \n",
    "    # Generate translations\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(tgt_seq_len), desc=\"Generating Translations\", position=0, ncols=240):\n",
    "            # Forward pass\n",
    "            output = transformer(tt, dt)\n",
    "\n",
    "            # Get the indices of the predicted tokens at the last position\n",
    "            pred_indices = torch.argmax(output[:, -1, :], dim=-1)\n",
    "\n",
    "            # Extract the predicted tokens as a list\n",
    "            pred_tokens = [port_field.vocab.itos[index.item()] for index in pred_indices]\n",
    "\n",
    "            # Append the predicted tokens to the target sequence\n",
    "            d.extend(pred_tokens)\n",
    "\n",
    "            # Stop if <EOS> is predicted\n",
    "            if (pred_indices == port_field.vocab.stoi[end]).any():\n",
    "                break\n",
    "            \n",
    "            # Update the target input tensor for the next step\n",
    "            dt = torch.cat([dt, pred_indices.unsqueeze(0)], dim=-1)\n",
    "            \n",
    "            # Enforce max_seq_length by keeping the last max_seq_length elements\n",
    "            dt = dt[:, -max_seq_len:]\n",
    "\n",
    "    # Convert the generated translation back to a string\n",
    "    translation = \" \".join(d)\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95313b5f-1fd6-45a1-9ba0-12af4cee0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"English Sentence : \", df['EN'][i])\n",
    "    print(\"Original Portugal Sentence : \", df['PT'][i])\n",
    "    print(\"Predicted Sentence : \", translate(df[\"EN\"][i]))\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6744f43-1d3d-4d4d-9173-f9b11e4db562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a73e6f-f1bf-4b10-93bc-7a4b413f6632",
   "metadata": {},
   "source": [
    "# Test Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea4a92-b8a6-47f2-bbfe-82686598e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"The weather is beautiful today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19dbdd-2cac-47f2-8749-2ffbf6762228",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I enjoy reading books in my free time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a811c-62c7-4973-92d4-f31a7c7e53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Here is the nearest coffee shop?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514499e-83c4-4ea4-b88e-45a5b00f85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Learning new languages broadens your perspective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60694e29-7f33-4f8a-ab1d-d9105c0addbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Can you recommend a good restaurant in the area?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277e9c2-47fd-40b7-a30d-6705788c774a",
   "metadata": {},
   "source": [
    "# Input Text and Get Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d5f0b-f3e9-461f-9967-c370a3fceb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_input():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Enter an English sentence (type 'exit' to quit): \")\n",
    "\n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting translation program.\")\n",
    "            break\n",
    "\n",
    "        # Use your existing translate function\n",
    "        translation = translate(user_input)\n",
    "\n",
    "        # Display the translation\n",
    "        print(f\"Translation: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8759d-e144-4360-b205-c46c5ae02fde",
   "metadata": {},
   "source": [
    "# Call The Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff50a1-05ba-48c0-ab2c-ac7031bcacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbe22d-8383-43b9-9fe6-779d76a5cb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-translator",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
